# LLM-Dï¼šåˆ†å¸ƒå¼æ¨ç†æ¶æ„ä»‹ç»ï¼ˆä¸€ï¼‰

æœ€è¿‘åœ¨å­¦ä¹ å’Œç ”ç©¶ LLM-Dï¼Œä½œä¸º Cloud-Native å’Œ LLM-Infra ä»ä¸šè€…å¯¹äºç±»ä¼¼æ–°å…´æŠ€æœ¯ä¸€ç›´æ¯”è¾ƒæ•æ„Ÿã€‚

æ¥ä¸‹æ¥æ‰“ç®—ä»¥å‡ ç¯‡æ–‡ç« æ¥åˆ†äº«ä¸‹æˆ‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚

- **LLM-D é¡¹ç›®ä»‹ç» ï¼ˆæœ¬æ–‡ï¼‰**
- LLM-D ä¸Šæ‰‹æŒ‡å—
- LLM-D ç»„ä»¶ä»‹ç»
- LLM-D å¼€æºè´¡çŒ®å‚ä¸

> çœŸçš„å­¦ä¸è¿‡æ¥ï¼Œè¿™ç¯‡æ–‡ç« å®Œæˆç”¨äº†ä¸€å‘¨...

## å†™åœ¨å‰é¢çš„è¯

### é¡¹ç›®å®šä½

LLM-D æ˜¯ç”± Red Hatã€ CoreWeaveã€Google Cloudã€NVIDIAã€IBM Research ç­‰è”åˆæ¨å‡ºçš„åŸºäº Kubernetes äº‘åŸç”Ÿåˆ†å¸ƒå¼æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡ç”Ÿæˆå¼ AI æ¨¡å‹æ¨ç†ä¸­çš„é«˜æˆæœ¬ã€é«˜å»¶è¿Ÿé—®é¢˜ï¼Œå…¶ç›®æ ‡æ˜¯å°†ç”Ÿäº§çº§ AI æ¨ç†èƒ½åŠ›æ ‡å‡†åŒ–ã€‚

### å¤§æ¨¡å‹æ¨ç†çš„æ ¸å¿ƒé—®é¢˜ä¸æŒ‘æˆ˜

1. ç®—åŠ›éœ€æ±‚çš„ç»“æ„æ€§è½¬å˜ï¼šæ¨ç†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å æ¯”æŒç»­å¢é•¿ï¼Œå¼•ç”³å‡ºå¯¹é«˜æ•ˆæ¨ç†æ¶æ„è¿«åˆ‡éœ€æ±‚ï¼›æ¨ç†ä¼˜åŒ–ç›®å‰éå¸¸ç«çƒ­ï¼Œå¹¶ä¸”çœ‹èµ·æ¥è¿˜æœ‰å¾ˆå¤§çš„ä¼˜åŒ–ç©ºé—´ã€‚
2. é›†ä¸­å¼æ¨ç†ç“¶é¢ˆï¼šæ¨¡å‹è§„æ¨¡è¿˜åœ¨æŒç»­æ‰©å¤§ï¼Œå¯¼è‡´èµ„æºéœ€æ±‚æ¿€å¢ï¼Œä¼ ç»Ÿæ¶æ„é¢ä¸´æˆæœ¬è¿‡é«˜ã€å»¶è¿Ÿè¿‡é•¿é—®é¢˜ï¼Œä¼˜åŒ–æˆæœ¬æ˜¯å…³é”®ã€‚
3. ä¼ä¸šè½åœ°éš¾ç‚¹ï¼šéœ€å¹³è¡¡æ€§èƒ½ï¼ˆSLOï¼‰ã€æˆæœ¬ï¼ˆTCOï¼‰ä¸å¼‚æ„ç¯å¢ƒå…¼å®¹æ€§ï¼ˆå°¤å…¶æ˜¯å›½äº§åŒ–çš„å¤§èƒŒæ™¯ä¸‹ï¼‰ã€‚

## LLM-D æ ¸å¿ƒæ¶æ„ä¸æŠ€æœ¯ç‰¹æ€§

### ç†è§£æ¨ç†çš„æœ¬è´¨å·®å¼‚

é¦–å…ˆï¼Œæˆ‘ä»¬è¦ç†è§£ LLM æ¨ç†æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œæ¯ä¸ªé˜¶æ®µæœ‰æˆªç„¶ä¸åŒçš„è®¡ç®—ç‰¹å¾ï¼š

```
Phase 1: Prefillï¼ˆé¢„å¡«å……ï¼‰
- è®¡ç®—ç‰¹å¾ï¼šO(nÂ²) attention è®¡ç®—ï¼Œè®¡ç®—å¯†é›†å‹
- å†…å­˜æ¨¡å¼ï¼šé¡ºåºå†™å…¥KV cache
- ç¡¬ä»¶éœ€æ±‚ï¼šé«˜FLOPSï¼ˆå¦‚H100: 989 TFLOPSï¼‰
- å…¸å‹è€—æ—¶ï¼š100-500msï¼ˆ8K contextï¼‰

Phase 2: Decodeï¼ˆè§£ç ï¼‰
- è®¡ç®—ç‰¹å¾ï¼šO(n) attention è®¡ç®—ï¼Œå†…å­˜å¯†é›†å‹
- å†…å­˜æ¨¡å¼ï¼šå¤§é‡KV cacheè¯»å–
- ç¡¬ä»¶éœ€æ±‚ï¼šé«˜å¸¦å®½ï¼ˆå¦‚L4: 300GB/sï¼‰
- å…¸å‹è€—æ—¶ï¼š30-50ms/token
```

- **Prefill é˜¶æ®µ**ï¼šå¤„ç† Promptï¼Œç”Ÿæˆ KV Cacheï¼Œå±äºè®¡ç®—å¯†é›†å‹ï¼ˆcompute-boundï¼‰ï¼Œä¸»è¦å½±å“ï¼ˆTTFTï¼‰
- **Decode é˜¶æ®µ**ï¼šåŸºäº KV Cache é€æ­¥ç”Ÿæˆè¾“å‡º tokenï¼Œå±äºå†…å­˜å¸¦å®½æ•æ„Ÿå‹ï¼ˆmemory-boundï¼‰ï¼Œä¸»è¦å½±å“ TPOTï¼‰

ä¼ ç»Ÿæ¶æ„å°†è¿™ä¸¤ä¸ªé˜¶æ®µç»‘å®šåœ¨åŒä¸€ GPU ä¸Šæ‰§è¡Œï¼Œå¯¼è‡´ä¸¥é‡çš„èµ„æºé”™é…ã€‚

#### LLM-D åˆ†å±‚æ¶æ„è®¾è®¡

```
EPPåè®®è·¯ç”±ç¼“å­˜æ„ŸçŸ¥è·¯ç”±RDMAä¼ è¾“KVç¼“å­˜æµå¼è¾“å‡ºå®¢æˆ·ç«¯è¯·æ±‚Endpoint/APIç½‘å…³Inference SchedulerPrefill PodDecode Podåˆ†å¸ƒå¼KVç¼“å­˜ç®¡ç†å™¨K8sæ§åˆ¶å™¨
```

### å…³é”®ç»„ä»¶

#### Inference Scheduler ï¼ˆæ¨ç†è°ƒåº¦å™¨ï¼‰

Inference Scheduler ä¸æ˜¯ç®€å•çš„è¯·æ±‚è´Ÿè½½å‡è¡¡å™¨ï¼Œè€Œæ˜¯æ·±å…¥ç†è§£æ¨ç†è¿‡ç¨‹çš„æ™ºèƒ½è´Ÿè½½è°ƒåº¦å™¨ï¼Œä¸»è¦èƒ½åŠ›åŒ…å«ï¼š

- åŠ¨æ€ä»»åŠ¡åˆ†é…ï¼šæ ¹æ®è¯·æ±‚ç±»å‹ï¼ˆPrefill/Decodeï¼‰è°ƒç”¨ EPP åè®®ï¼ˆEndpoint Picker Protocolï¼‰é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹ã€‚
- èµ„æºæ„ŸçŸ¥è°ƒåº¦ï¼šåŸºäºèŠ‚ç‚¹è´Ÿè½½ã€KV ç¼“å­˜å‘½ä¸­ç‡ã€ç¡¬ä»¶ç‰¹æ€§ï¼ˆGPU ç®—åŠ›/ å†…å­˜å¸¦å®½ï¼‰åŠ¨æ€è¯„åˆ†ã€‚
- PD åˆ†ç¦»ï¼šåˆ†ç¦»è®¡ç®—å¯†é›†å‹é¢„å¡«å……ï¼ˆPrefillï¼‰ä¸å†…å­˜å¯†é›†å‹è§£ç ï¼ˆDecodeï¼‰ï¼Œ**æ”¯æŒç‹¬ç«‹æ‰©ç¼©å®¹**ã€‚

```
classInferenceScheduler:
    defroute_request(self, request):
        # 1. è¯·æ±‚ç‰¹å¾åˆ†æ
        prompt_tokens = tokenizer.encode(request.prompt)
        expected_output = self.predict_output_length(prompt_tokens)

        # 2. ç¼“å­˜å‘½ä¸­ç‡é¢„æµ‹
        cache_hit_rate = self.cache_analyzer.predict_hit_rate(
            prompt_tokens[:512]  # prefixåˆ†æ
        )

        # 3. EPPåè®®è¯„åˆ†
        prefill_scores = {}
        for node inself.prefill_nodes:
            score = self.epp_score(
                node_load=node.current_load,
                cache_affinity=cache_hit_rate[node.id],
                network_distance=self.get_latency(node),
                gpu_memory_available=node.free_memory
            )
            prefill_scores[node] = score

        # 4. é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
        best_prefill = max(prefill_scores, key=prefill_scores.get)

        # 5. é¢„åˆ†é…decodeèµ„æº
        decode_node = self.reserve_decode_node(
            expected_tokens=expected_output,
            sla_requirements=request.sla
        )

        return best_prefill, decode_node
```

#### Prefill/Decode Pod (PD åˆ†ç¦»)

æ­£å¦‚å‰é¢æ‰€è¯´ï¼Œå½“å‰ PD æ··åˆä¸èƒ½å¾ˆå¥½çš„å‘æŒ¥ç¡¬ä»¶æ€§èƒ½ï¼ŒLLM-D å®šä¹‰äº† Distributed Inferenceï¼Œå¾ˆå¥½çš„å°†æ¨ç†è¿‡ç¨‹æ‹†æˆä¸¤ä¸ªç‹¬ç«‹éƒ¨ç½²çš„å·¥ä½œè´Ÿè½½ï¼š

```
apiVersion: llm-d.ai/v1
kind:InferenceCluster
metadata:
name:llama-70b
spec:
model:
    name:meta-llama/Llama-2-70b-chat
    quantization:fp16

prefill:
    replicas:4
    hardware:
      gpu:nvidia.com/gpu-a100-80gb
      count:2
    optimization:
      flashAttention:v2
      fusedKernels:true

decode:
    replicas:16
    hardware:
      gpu:nvidia.com/gpu-l4-24gb
      count:1
    optimization:
      pagedAttention:true
      kvCompression:0.5

transport:
    protocol:nixl
    backend:rdma
    bandwidth:200Gbps

cache:
    type:distributed
    backend:lmcache
    capacity:2TB
    eviction:lru
    tiering:
      -type:gpu
        capacity:320GB
      -type:cpu
        capacity:1TB
      -type:nvme
        capacity: 10TB
```

| ç»„ä»¶        | åŠŸèƒ½                                                                     | ä¼˜åŒ–æ–¹å‘                           |
| :---------- | :----------------------------------------------------------------------- | :--------------------------------- |
| Prefill Pod | å¤„ç†å®Œæ•´æç¤ºè¯ï¼Œç”Ÿæˆé¦– Token åŠ KV ç¼“å­˜                                  | é«˜ç®—åŠ› GPUï¼ˆå¦‚ H100ã€A100ï¼‰        |
| Decode Pod  | åŸºäº KV ç¼“å­˜æµå¼ç”Ÿæˆåç»­ Token                                           | é«˜å†…å­˜å¸¦å®½ç¡¬ä»¶ï¼ˆå¦‚ä¸“ç”¨æ¨ç†åŠ é€Ÿå™¨ï¼‰ |
| Transport   | è´Ÿè´£åœ¨ Prefill ä¸ Decode Pod ä¹‹é—´ä¼ é€’ KV Cacheï¼Œä»¥æ”¯æŒé˜¶æ®µåˆ†ç¦»å’Œç¼“å­˜å…±äº« | NIXL                               |
| Cache       | ç”¨äºå­˜å‚¨ã€å¤ç”¨å’Œè°ƒåº¦ KV Cacheï¼Œæå‡æ¨ç†æ•ˆç‡ä¸ååï¼ŒåŒæ—¶å‡å°‘é‡å¤è®¡ç®—      | LMCache                            |

- åœ¨ Prefill ç”Ÿæˆ KV ç¼“å­˜åï¼Œéœ€é€šè¿‡ RDMA/NVLink æˆ– NIXL é«˜æ€§èƒ½ä¼ è¾“åº“æ¯«ç§’çº§ä¼ è¾“è‡³ Decode èŠ‚ç‚¹ ï¼ˆå¯¹ç½‘ç»œæ€§èƒ½è¦æ±‚å¾ˆé«˜ï¼‰
- llm-d å€ŸåŠ© vLLM çš„å¯æ’æ‹” KV Connector API å®ç° Prefill ä¸ Decode ä¹‹é—´çš„ KV Cache ä¼ é€’

#### EPP åè®®

Inference Scheduler æä¾›äº†ä¸€ä¸ªâ€œç«¯ç‚¹é€‰æ‹©å™¨ï¼ˆEPPï¼‰â€ç»„ä»¶ï¼Œé€šè¿‡è¿™ç§æ–¹å¼å°†ä¼ å…¥çš„æ¨ç†è¯·æ±‚è°ƒåº¦åˆé€‚èŠ‚ç‚¹ä¸Šï¼Œå®ç°äº†æ™ºèƒ½åç«¯é€‰æ‹© Pods çš„èƒ½åŠ›ã€‚

![Image](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

EPP æ˜¯ GIE ä¸­æ’ä»¶åŒ–è·¯ç”±æ¥å£åè®®ï¼Œé€šè¿‡å®ƒå¯ä»¥è‡ªå®šä¹‰è¿‡æ»¤å™¨ï¼ˆfiltersï¼‰å’Œæ‰“åˆ†å™¨ï¼ˆscorersï¼‰ï¼Œ
LLM-D å€Ÿæ­¤æ¥å£å°†å…¶è°ƒåº¦é€»è¾‘é›†æˆè¿› GIEï¼Œåˆ©ç”¨äº† GIE çš„å¯æ’æ‹”æ¶æ„ï¼Œå°†å…¶ä½œä¸ºæ¨ç†è°ƒåº¦çš„å…¥å£ã€‚

> GIE(Gateway API Inference Extension) æ˜¯é’ˆå¯¹ AI/LLM æ¨ç†æµé‡è®¾è®¡çš„ Kubernetes Gateway API æ‰©å±•ï¼Œ
> é›†æˆäº†å¯¹æ¨¡å‹è·¯ç”±ã€å‰ç¼€ç¼“å­˜æ„ŸçŸ¥ï¼ˆprefix-cache-aware routingï¼‰ã€æœåŠ¡ä¼˜å…ˆçº§ç­‰è°ƒåº¦é€»è¾‘çš„æ”¯æŒã€‚

LLM-D ä¼šæ”¶é›†æ¥è‡ª vLLM çš„è¿è¡Œæ—¶æ•°æ®ï¼ˆå¦‚å“ªäº› KV cache åœ¨å“ªä¸ªå®ä¾‹ç¼“å­˜?ã€å„å®ä¾‹è´Ÿè½½æƒ…å†µ? ç­‰ï¼‰ï¼Œæ®æ­¤å®æ–½è¿‡æ»¤ä¸æ‰“åˆ†ï¼Œä»è€ŒåŠ¨æ€å†³ç­–è¯·æ±‚çš„æœ€ç»ˆè·¯ç”±ç›®æ ‡ã€‚

EPPï¼ˆEndpoint Picker Protocolï¼‰çš„æ ¸å¿ƒè¯„åˆ†ç®—æ³•ï¼š

```
Score = Î± * (1 - LoadFactor) +
        Î² * CacheAffinity +
        Î³ * (1 / NetworkLatency) +
        Î´ * MemoryAvailable

å…¶ä¸­ï¼š
- Î± = 0.3ï¼ˆè´Ÿè½½æƒé‡ï¼‰
- Î² = 0.4ï¼ˆç¼“å­˜æƒé‡ï¼Œæœ€é‡è¦ï¼‰
- Î³ = 0.2ï¼ˆç½‘ç»œæƒé‡ï¼‰
- Î´ = 0.1ï¼ˆå†…å­˜æƒé‡ï¼‰
```

> https://github.com/llm-d/llm-d-inference-scheduler/blob/main/docs/architecture.md

#### NIXL é«˜æ€§èƒ½ä¼ è¾“å±‚ ï¼ˆTransportï¼‰

LLM-D ä¸»è¦åˆ©ç”¨ vLLM çš„ KVConnector å’Œ NIXL æ„å»º PD åˆ†ç¦»æ¨ç†çš„æµæ°´çº¿ï¼Œå¹¶åµŒå…¥åˆ° Inference Schedulerï¼Œå®ç°ç¼“å­˜æ„ŸçŸ¥è°ƒåº¦ã€‚

NIXLï¼ˆNVIDIA Inference Transfer Libraryï¼‰æ˜¯ PD åˆ†ç¦»çš„å…³é”®åº•å±‚ä¼ è¾“åº“ï¼Œè´Ÿè´£é«˜é€Ÿå¼‚æ­¥åœ°åœ¨ GPU æˆ–ä¸åŒå­˜å‚¨å±‚ä¹‹é—´ç§»åŠ¨ KV cacheï¼Œ
ä¸»è¦å®ç°æ˜¯é€šè¿‡ NixlConnector ä¸ vLLM é€šä¿¡å°† KV Cache buffer è½¬ç§»ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶äº’è”æ–¹å¼ï¼Œå’Œè‡ªé€‚åº”é€‰æ‹©äº’è”æ–¹å¼ï¼Œç¡®ä¿ä¼ è¾“ä½å»¶è¿Ÿä¸é«˜ååã€‚

| **é˜¶æ®µ**         | **ä¸»è¦èŒè´£ä¸ç»„ä»¶**                                                                     |
| :--------------- | :------------------------------------------------------------------------------------- |
| **Prefill é˜¶æ®µ** | vLLM ä½¿ç”¨ KVConnectorï¼ˆå¦‚ NixlConnectorï¼‰è¾“å‡º KV Cache bufferï¼Œ[å¯èƒ½ç»è¿‡ LMCache èšåˆ] |
| **ä¼ è¾“å±‚**       | NIXL è´Ÿè´£ buffer çš„è·¨å®ä¾‹é«˜æ•ˆä¼ è¾“ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶äº’è¿æ–¹å¼ã€‚                              |
| **Decode é˜¶æ®µ**  | æ¥æ”¶ bufferï¼Œæ¢å¤ KV Cacheï¼Œç”¨äºåç»­ç”Ÿæˆ tokenã€‚                                       |

**NIXL æ ¸å¿ƒçš„äº®ç‚¹**æ˜¯é€šè¿‡ä¸€ä¸ªå¤šåŠŸèƒ½ API æä¾›è·¨å„ç§å†…å­˜ç±»å‹çš„ç»Ÿä¸€æŠ½è±¡ï¼Œéšè—äº†é¢å¤–çš„åç«¯ç»†èŠ‚ï¼Œå¦‚è¿æ¥ç®¡ç†ã€å¯»å€æ–¹æ¡ˆå’Œå†…å­˜ç‰¹æ€§ç­‰ï¼Œç®€åŒ–äº†ä½¿ç”¨æˆæœ¬ã€‚

> https://github.com/ai-dynamo/nixl
>
> https://github.com/ai-dynamo/nixl/blob/main/docs/nixl.md

#### 3. LMCache åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿ

LMCache æä¾›äº†å¤šçº§ç¼“å­˜æ¶æ„ï¼Œåœ¨ vLLM ç”Ÿæˆä¸Šä¸‹æ–‡ KV Cache ä¹‹åï¼ŒLMCache èƒ½æœ‰æ•ˆç¼“å­˜è¿™äº› KVã€‚æ”¯æŒåŠ¨æ€é€‰æ‹©ç¼“å­˜ä½ç½®ï¼ŒåŒ…æ‹¬ GPU æ˜¾å­˜ã€ä¸»æœºå†…å­˜ï¼ˆDRAMï¼‰ã€æœ¬åœ°ç£ç›˜ï¼Œç”šè‡³è¿œç¨‹å­˜å‚¨ï¼Œä»è€Œå‡å°‘é‡å¤è®¡ç®—æ‰€éœ€çš„ GPU å‘¨æœŸã€‚

ç›®å‰ LMCache å·²æˆä¸º LLM-D é»˜è®¤çš„ KV Cache å±‚ï¼Œè´Ÿè´£ç¼“å­˜ã€å¤ç”¨å¹¶è·¨ç¡¬ä»¶/ å®ä¾‹ä¼ é€’ KV Cacheã€‚

```
class LMCache:
    def__init__(self):
        self.cache_hierarchy = [
            GPUCache(capacity="320GB", latency="100ns"),
            CPUCache(capacity="1TB", latency="100Î¼s"),
            NVMeCache(capacity="10TB", latency="10ms"),
            S3Cache(capacity="unlimited", latency="100ms")
        ]

    defget(self, key, tokens_needed):
        # 1. è®¡ç®—ç¼“å­˜æŸ¥æ‰¾ç­–ç•¥
        for cache_tier inself.cache_hierarchy:
            if cache_tier.contains(key):
                # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦æå‡åˆ°æ›´å¿«çš„å±‚çº§
                ifself.should_promote(key, cache_tier):
                    self.promote_to_faster_tier(key, cache_tier)

                # 3. å¼‚æ­¥é¢„å–å¯èƒ½éœ€è¦çš„åç»­å—
                self.prefetch_adjacent_blocks(key, tokens_needed)

                return cache_tier.get(key)

        returnNone# Cache miss

    defput(self, key, value):
        # æ™ºèƒ½å†³å®šå­˜å‚¨å±‚çº§
        tier = self.select_tier_by_access_pattern(key)
        tier.put(key, value)

        # å¼‚æ­¥å¤åˆ¶åˆ°ä¸‹çº§å­˜å‚¨
        self.async_replicate_to_lower_tiers(key, value, tier)
```

**æ ¸å¿ƒç‰¹è‰²**ï¼š

| **é˜¶æ®µ / åŠŸèƒ½**          | **ä½œç”¨**                                                        |
| :----------------------- | :-------------------------------------------------------------- |
| Prefill åç¼“å­˜å­˜å‚¨       | æŒä¹…åŒ– KV Cacheï¼Œæ”¾å…¥ GPUã€DRAMã€ç£ç›˜ã€è¿œç¨‹å­˜å‚¨ç­‰å¤šä¸ªå±‚çº§       |
| ç¼“å­˜å¤ç”¨ï¼ˆå¤šè½®å¯¹è¯ï¼‰     | é¿å…é‡å¤è®¡ç®— KV Cacheï¼Œæé«˜å“åº”é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨æ•ˆç‡               |
| åˆ†å¸ƒå¼ Prefill ä¸ Decode | ä¸ vLLM çš„ KVConnector å’Œ NIXL åä½œï¼Œå®ç°é«˜æ•ˆç¼“å­˜ä¼ è¾“å’Œé˜¶æ®µåˆ†ç¦» |
| éå‰ç¼€ç¼“å­˜æ”¯æŒ           | æé«˜å¤šè½®å¯¹è¯æˆ–ä¸Šä¸‹æ–‡å¤æ‚åœºæ™¯ä¸­çš„å‘½ä¸­ç‡                          |

> https://github.com/LMCache/LMCache

## æ ¸å¿ƒä¼˜åŒ–äº®ç‚¹

| æŠ€æœ¯                                    | æ•ˆæœ                                     | **é€‚ç”¨åœºæ™¯**                  |
| :-------------------------------------- | :--------------------------------------- | :---------------------------- |
| **å‰ç¼€ç¼“å­˜æ„ŸçŸ¥è·¯ç”±ï¼ˆPrefix Cache)**     | TTFTï¼ˆé¦– Token æ—¶é—´ï¼‰å¤§å¹…é™ä½            | å¯¹ SLO æœ‰é«˜è¦æ±‚çš„å®æ—¶äº¤äº’åœºæ™¯ |
| **å˜ä½“è‡ªåŠ¨æ‰©ç¼©ï¼ˆVariant Autoscalingï¼‰** | æŒ‰éœ€ä¼¸ç¼© Prefill/Decode ç»„ï¼Œæœ‰æ•ˆæ§åˆ¶æˆæœ¬ | æµé‡æ³¢åŠ¨å¤§çš„åœºæ™¯              |

### Prefix Cache æ„ŸçŸ¥è·¯ç”±

é€šè¿‡åˆ†æè¯·æ±‚çš„ **Prefix** ç›¸ä¼¼åº¦ï¼Œæ™ºèƒ½è·¯ç”±åˆ°ç¼“å­˜å‘½ä¸­ç‡æœ€é«˜çš„èŠ‚ç‚¹ï¼š

```
def prefix_aware_routing(request, nodes):
    # æå–å‰512 tokensä½œä¸ºprefix signature
    prefix = request.tokens[:512]
    prefix_hash = hash(prefix)

    # æŸ¥è¯¢å„èŠ‚ç‚¹çš„ç¼“å­˜çŠ¶æ€
    cache_stats = []
    for node in nodes:
        hit_rate = node.cache_bloom_filter.estimate_hit_rate(prefix_hash)
        cache_stats.append({
            'node': node,
            'hit_rate': hit_rate,
            'saved_compute': hit_rate * len(prefix) * ATTENTION_FLOPS
        })

    # é€‰æ‹©æ”¶ç›Šæœ€å¤§çš„èŠ‚ç‚¹
    best_node = max(cache_stats, key=lambda x: x['saved_compute'])

    # å¦‚æœå‘½ä¸­ç‡>80%ï¼ŒTTFTå¯é™ä½70%
    if best_node['hit_rate'] > 0.8:
        estimated_ttft = BASE_TTFT * 0.3
    else:
        estimated_ttft = BASE_TTFT

    return best_node['node'], estimated_ttft
```

### å˜ä½“è‡ªåŠ¨æ‰©ç¼©ï¼ˆVariant Autoscalingï¼‰

**æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹å¾åŠ¨æ€è°ƒæ•´** Prefill/Decode é…æ¯”ï¼š

```
apiVersion: autoscaling/v2
kind:HorizontalPodAutoscaler
metadata:
name:llm-d-autoscaler
spec:
scaleTargetRef:
    apiVersion:llm-d.ai/v1
    kind:InferenceService
    name:production-service

minReplicas:
    prefill:2
    decode:4

maxReplicas:
    prefill:10
    decode:40

metrics:
    -type:Custom
      custom:
        metric:
          name:prefill_queue_depth
        target:
          type:Value
          value:10# é˜Ÿåˆ—æ·±åº¦è¶…è¿‡10åˆ™æ‰©å®¹

    -type:Custom
      custom:
        metric:
          name:decode_token_latency_p95
        target:
          type:Value
          value:50ms# P95å»¶è¿Ÿè¶…è¿‡50msåˆ™æ‰©å®¹

behavior:
    scaleUp:
      policies:
        -type:Percent
          value:100# å¿«é€Ÿæ‰©å®¹ï¼Œæ¯æ¬¡ç¿»å€
          periodSeconds:15
    scaleDown:
      policies:
        -type:Percent
          value:10# ç¼“æ…¢ç¼©å®¹ï¼Œæ¯æ¬¡å‡10%
          periodSeconds: 300
```

### ä¸»æµæ¨ç†æ¡†æ¶å¯¹æ¯”

### å¯èƒ½æœ‰ä¸å‡†ç¡®çš„åœ°æ–¹ï¼ŒçŸ¥è¯†æœ‰é™ä¸”æŠ€æœ¯å‘å±•å¤ªå¿«ï¼Œæ¬¢è¿å¤§å®¶æŒ‡æ­£ã€‚

| ç‰¹æ€§            | LLM-D          | vLLM            | TensorRT-LLM | SGLang  |
| :-------------- | :------------- | :-------------- | :----------- | :------ |
| P/D è§£è€¦        | âœ… åŸç”Ÿæ”¯æŒ    | âš ï¸ å®éªŒæ€§       | âŒ           | âš ï¸ éƒ¨åˆ† |
| åˆ†å¸ƒå¼ KV Cache | âœ…LMCache      | âŒ              | âŒ           | âš ï¸ æœ‰é™ |
| K8s åŸç”Ÿ        | âœ…CRD+Operator | âš ï¸ éœ€è¦é¢å¤–é…ç½® | âŒ           | âŒ      |
| å¼‚æ„ç¡¬ä»¶        | âœ…CPU+GPU æ··åˆ | âš ï¸ ä¸»è¦ GPU     | âŒ ä»… NVIDIA | âš ï¸      |
| æ™ºèƒ½è·¯ç”±        | âœ…EPP åè®®     | âŒ              | âŒ           | âš ï¸ åŸºç¡€ |
| ç¼“å­˜æ„ŸçŸ¥è°ƒåº¦    | âœ… åŸç”Ÿ        | âš ï¸ æ‰‹åŠ¨         | âŒ           | âŒ      |

## æ€»ç»“ä¸å±•æœ›

LLM-D åœ¨æŠ€æœ¯ä¸Šçš„äº®ç‚¹ï¼šPD åˆ†ç¦»ã€ç¼“å­˜æ„ŸçŸ¥è·¯ç”±ã€äº‘åŸç”ŸåŒ–çš„æ ¸å¿ƒç‰¹è‰²ï¼Œä¸ºå¤§æ¨¡å‹æ¨ç†å¸¦æ¥æ–°çš„æ¨ç†ä¼˜åŒ–æ€è·¯ï¼ŒåŒç±»çš„é¡¹ç›®ä¹Ÿæ˜¯éå¸¸å¤šï¼Œè¶³è§å¤§æ¨¡å‹æ¨ç†è¿˜æ˜¯æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚

å½“ç„¶ï¼ŒLLM-D ç›®å‰å‘å±•è¿˜åœ¨æ—©æœŸï¼›ä»é•¿è¿œè§’åº¦æ¥çœ‹ï¼Œæ¨åŠ¨ AI æ¨ç†ä»â€œé›†ä¸­å¼å•ç‚¹ä¼˜åŒ–â€è½¬å‘â€œåˆ†å¸ƒå¼æ ‡å‡†åŒ–â€ï¼Œå¯¹æœªæ¥çš„è‡ªåŠ¨å¹¶è¡Œä¸å®‰å…¨åä½œæ–¹å‘ä¹Ÿå…·å¤‡å¾ˆé«˜æ½œåŠ›ï¼Œæ˜¯å€¼å¾—å…³æ³¨å’Œå‚ä¸çš„å¼€æºåŸºç¡€è®¾æ–½é¡¹ç›®ã€‚

- ğŸ‘‰ é¡¹ç›®ä¸»é¡µï¼šhttps://llm-d.ai
- ğŸ‘‰ GitHub ä»“åº“ï¼šhttps://github.com/llm-d/llm-d
